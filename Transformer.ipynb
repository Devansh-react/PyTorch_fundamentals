{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+Ez6wXptmSjJxOrR3hSRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devansh-react/PyTorch_fundamentals/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICO61jMf210i",
        "outputId": "89b01966-503b-4d25-c507-32c745a4280e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-11 20:07:33--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-02-11 20:07:33 (40.9 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\",'r',encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text[:2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wi_p1zLT3Hmt",
        "outputId": "6a78c422-2a9e-4b3c-89ac-ed5db7e0af4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitude of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You must in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the city\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst C\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars =sorted(list(set(text)))\n",
        "print(len(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgSrwnqN3ZdB",
        "outputId": "ef4a3eec-f353-40c8-a82e-003218dab617"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SybxpC5y4o83",
        "outputId": "43a7e2ba-8a09-4628-ef65-a2dbecc4a084"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
        "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n"
      ],
      "metadata": {
        "id": "sju8k5tW4YrX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(enc.encode(text),dtype=torch.long)\n",
        "print(f\"length of dataset in characters: {len(data)}& datatype is {data.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEWbsQ6t5BTW",
        "outputId": "67a94c17-9f40-402c-f784-6f828d47934d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 297606& datatype is torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3wJF6wK63Ey",
        "outputId": "ff7dbe70-770c-477a-a7f6-95ca34f8aca2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  7127,  84479,    734,  13036,    581,  18988,   1062,   6544,     11,\n",
            "          9598,    668,  10591,    364,   2594,    734, 116872,     11,  10591,\n",
            "           364,   7127,  84479,    734,   3575,    553,    722,  33944,   7542,\n",
            "           316,   1076,   1572,    316,   2079,   1109,   1715,   2594,    734,\n",
            "         80773,     13,  33944,    364,   7127,  84479,    734,   7127,     11,\n",
            "           481,   1761, 149492,    385,   3145, 137928,    382,  20915,  20935,\n",
            "           316,    290,   1665,    364,   2594,    734,   2167,   1761,   1507,\n",
            "            11,    581,   1761,   1507,    364,   7127,  84479,    734,  12845,\n",
            "           765,  15874,   2395,     11,    326,  22782,    679,  33994,    540,\n",
            "          1039,   2316,   3911,    558,   3031,   1507,    261,  75722,   1715,\n",
            "          2594,    734,   3160,    945,  11695,    402,   1507,     26,   1632,\n",
            "           480,    413,   4167,     25,   4194,     11,   4194,   1703,  17422,\n",
            "         84479,    734,   5045,   2195,     11,   1899,  19466,    364,   7127,\n",
            "         84479,    734,   2167,    553,  83982,  12530,  19466,     11,    290,\n",
            "          2506,   2740,  11413,   1899,    558,   4827,  20515,   1512,   2302,\n",
            "          1348,    402,   1481,  61327,    765,     25,    538,   1023,    198,\n",
            "         83527,  14376,    765,    889,    290,   2539,  32844,    536,     11,\n",
            "          2049,    480,   1504,    198,   2078,   8795,    747,     11,    581,\n",
            "          3572,  11915,   1023,  91895,    765,   5396,   1151,    307,   8293,\n",
            "          1023,   2411,    581,    553,   3101,  36203,     25,    290,    505,\n",
            "           934,    436,    484,    198,   2857, 111894,    765,     11,    290,\n",
            "          2817,    328,   1039, 117394,     11,    382,    472,    448,    198,\n",
            "         69660,    316,   5024,   1096,   1043,  50280,     26,   1039,    198,\n",
            "            82,   3299,    766,    382,    261,  11621,    316,   1373,   9024,\n",
            "           765,  72519,    495,    483,    198,    401,    275,  13396,     11,\n",
            "         16466,    581,   5025,    428,   6861,     25,    395,    290,  52901,\n",
            "          1761,    357,    198,     82,  42583,    495,    306,  53080,    395,\n",
            "         19544,     11,    625,    306,  79547,    395,  72519,    364,  17422,\n",
            "         84479,    734,  43762,    481,  18988,   6980,   4372, 149492,    385,\n",
            "          3145, 137928,   1715,   2594,    734, 141068,   2395,   1577,     25,\n",
            "         19016,    261,   1869,   6446,    316,    290,   5355,  59531,    364,\n",
            "         17422,  84479,    734,  49377,    481,   1412,   3581,    501,    853,\n",
            "          4167,    395,   1232,   4931,   1715,   7127,  84479,    734,  29061,\n",
            "          1775,     26,    326,   2023,    413,   3100,    316,   3644,   2395,\n",
            "          1899,    198,  22869,   8024,     11,    889,    484,    501,  15236,\n",
            "         11166,    483,   2447,  15164,    364,  17422,  84479,    734,     45,\n",
            "           356,     11,    889,  10591,    625,  73880,    423,    364,   7127,\n",
            "         84479,    734,     40,   2891,  47329,    481,     11,   1412,    501,\n",
            "         94658,   4167, 125114,     11,    501,   2242,    198,    278,    316,\n",
            "           484,   1268,     25,   5495,  10143,  21675,   2786,  54634,   1966,\n",
            "           665,    413,    198,   3252,    316,   2891,    480,    673,    395,\n",
            "          1232,   4931,    501,   2242,    480,    316,    198,  48576,   1232,\n",
            "         10005,    326,    316,    413,  53958,  15164,     26,   1118,    501,\n",
            "           198,    276,     11,   1952,   7892,    290,  66032,    328,   1232,\n",
            "         74782,    364,  17422,  84479,    734,   4827,    501,   6284,   1652,\n",
            "           306,   1232,   7867,     11,    481,   3527,    261,    198,   2554,\n",
            "           306,   2395,     13,   1608,   2804,    306,    860,   2006,   2891,\n",
            "           501,    382,   8885,    292,    784,    364,   7127,  84479,    734,\n",
            "          3335,    357,   2804,    625,     11,    357,   1309,    625,    413,\n",
            "        190982,    328,  97313,    307,    273,  94658,  92346,     11,    483,\n",
            "         78529,     11,    316,  30796,    306, 100556,    558,   4827,    641,\n",
            "         16513,    553,   1879,     30,    623,   1273,   4307,    293,      6,\n",
            "           290,   5030,    198,    276,  79645,     25,   4436,   5092,    581,\n",
            "           550,   1365,   2105,     30,    316,    290,  61820,   1703,   2594,\n",
            "           734,  37788,     11,   3063,    364,   7127,  84479,    734,  35689,\n",
            "             0,   1218,   5124,   2105,   1715,  17422,  84479,    734,     54,\n",
            "         66107,  10841,  84002,  26053, 118571,     26,   1001,    484,  94658,\n",
            "          3324,  10869,    198,   3086,   1665,    364,   7127,  84479,    734,\n",
            "         98880,   1001,  13734,   4951,     25,   1481,    722,    290,   2867,\n",
            "          1504,    813,   1703, 125091,   1262,     40,   3042,    734,   4827,\n",
            "          1101,    885,     11,    922,   4931,   2712,     11,    306,   1803,\n",
            "            30,   1919,    810,    481,    198,   2886,  69923,    326,  27661,\n",
            "            30,    623,   7165,     30,  10591,     11,    357,  33836,    481,\n",
            "           364,   7127,  84479,    734,   7942,   2413,    382,    625,  17927,\n",
            "           316,    290, 166420,     26,   1023,    679,    198,  26355,  22045,\n",
            "          3321,    495, 160236,   1412,    581,  36988,    316,    621,    412,\n",
            "         13980,   1954,  22782,   2356,    461,    347,    306,  95695,     13,\n",
            "          3164,   2891,  12530,    198,     82,   2136,    914,    679,   5532,\n",
            "        181534,     25,   1023,   8712,   1761,    581,    198,  35723,   5532,\n",
            "         21157,   3101,    364, 125091,   1262,     40,   3042,    734,  13903,\n",
            "            11,  55627,     11,    922,   1899,   5664,     11,  13336,  13734,\n",
            "         71541,    412,  17886,    481,  59427,  96585,   1715,   7127,  84479,\n",
            "           734,   2167,   6284,     11,  21583,     11,    581,    553, 178433,\n",
            "          4279,    364, 125091,   1262,     40,   3042,    734,     40,   5485,\n",
            "           481,     11,   5664,     11,   1645,  77860,   2631,    198,  15334,\n",
            "           290,   2506,   2740,  11413,    328,    481,     13,   2214,    634,\n",
            "         10648,    412,   9719,  25077,    306,    495,    334,   7087,     11,\n",
            "           481,   1340,    472,   1775,    198, 109630,    540,    290,  30696,\n",
            "           483,    634,    420,   6808,    472,  19604,   1373,    198, 141068,\n",
            "           290,  18753,   2608,     11,  12119,   4165,    738,    402,    198,\n",
            "           976,   2006,    480,   6948,     11,  89138,   4325,  26791,   4396,\n",
            "          6229,    198,   2566,    945,   5532,   3461,    472,  11093,   1572,\n",
            "           665,   4862,    198,  70817,    306,    634,  58313,   2581,     13,\n",
            "          2214,    290,    334,   7087,    412,    976,  52901,     11,    625,\n",
            "           290,   2506,   2740,  11413,     11,   1520,    480,     11,    326,\n",
            "           198,   9719,  54434,    316,   1373,     11,    625,  21157,     11,\n",
            "          2804,   1652,     13,   1667,    552,    412,   3575,    553,  68474,\n",
            "           656, 117183,    536,    198,   1139,   3990,   1919,    945, 134910,\n",
            "           481,     11,    326,    481,   1925,   9330,    198,    976,   1392,\n",
            "          1782,    293,      6,    290,   2608,     11,   1218,   2631,    395,\n",
            "           481,   1299,  73361,    412,   5958,    481,  77365,   1373,    472,\n",
            "         33974,    364,   7127,  84479,    734,  32158,    395,    765,      0,\n",
            "          6432,     11,  22476,      0,   3164,    453,  88929,  65750,    395,\n",
            "           765,    198,  54103,     25,  17170,    765,    316,   2079,   1109,\n",
            "            11,    326,   1043,   4897,   4559,  26940,    198,    798,  44072,\n",
            "           483,  35285,     26,   1520,   1648, 147653,    395,    765,   4248,\n",
            "            11,    316,    198,  44859,    765,  16003,     26, 139319,   8424,\n",
            "          1062, 109488,   1330,    198,    376,  46484,   4372,    290,  10358,\n",
            "            11,    326,   3587,    945,    198, 165772,   6776, 139404,   8424,\n",
            "            11,    316,  13464,    869,    326,  10253,    524,    198,   3086,\n",
            "         12530,     13,   1843,    290,  45576,  11237,    765,    625,    869,\n",
            "            11,   1023,    738,     26,    326,    198,  31813,    885,    722,\n",
            "           290,   3047,   1023,  16387,    765,    364, 125091,   1262,     40,\n",
            "          3042,    734,  94545,    481,   2804,    198,  19077,    436,  96585,\n",
            "        159740,  45170,  73880,    412,   2251,    413,  33385,    328, 190201,\n",
            "            13,    357,   8712,   5485,    481,    198,     32,   7264,  26552,\n",
            "            25,    480,   1340,    413,    481,    679,  10542,    480,    307,\n",
            "          7943,     11,   3630,    480,  22398,    922,   9676,     11,    357,\n",
            "           738,  34365,    198,   1385, 107915,    461,     83,    261,   3389,\n",
            "           945,    364,   7127,  84479,    734,  16936,     11,  17291,   9598,\n",
            "           480,     11,  21583,     25,   5073,    481,   2804,    625,   2411,\n",
            "           316])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fraction  =  int(0.8*len(data))\n",
        "train_data = data[:fraction]\n",
        "test_data = data[fraction:]\n",
        "print(f\"len of traindata:{len(train_data)}\")\n",
        "print(f\"len of testdata:{len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc1xNrku66wh",
        "outputId": "db5c2863-16fc-47a0-9b54-7882df7c07ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of traindata:238084\n",
            "len of testdata:59522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 10\n",
        "train_data[:10+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ADBVTZpAANp",
        "outputId": "72112275-98f3-4e32-fbb9-c3ea17a9d2f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 7127, 84479,   734, 13036,   581, 18988,  1062,  6544,    11,  9598,\n",
              "          668])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  target = x[:t+1]\n",
        "  context = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJC5rpL0AraA",
        "outputId": "a2df8656-d49f-4e4b-eecd-3b5c7c397dc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is 84479 the target is tensor([7127])\n",
            "when input is 734 the target is tensor([ 7127, 84479])\n",
            "when input is 13036 the target is tensor([ 7127, 84479,   734])\n",
            "when input is 581 the target is tensor([ 7127, 84479,   734, 13036])\n",
            "when input is 18988 the target is tensor([ 7127, 84479,   734, 13036,   581])\n",
            "when input is 1062 the target is tensor([ 7127, 84479,   734, 13036,   581, 18988])\n",
            "when input is 6544 the target is tensor([ 7127, 84479,   734, 13036,   581, 18988,  1062])\n",
            "when input is 11 the target is tensor([ 7127, 84479,   734, 13036,   581, 18988,  1062,  6544])\n",
            "when input is 9598 the target is tensor([ 7127, 84479,   734, 13036,   581, 18988,  1062,  6544,    11])\n",
            "when input is 668 the target is tensor([ 7127, 84479,   734, 13036,   581, 18988,  1062,  6544,    11,  9598])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else test_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).long() # Convert to long\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).long() # Convert to long\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----'*50)\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEYVhVfkBa_4",
        "outputId": "9c618304-2a5c-4394-f297-e73cc425bd29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[   976,   5030,    382,   1775,  16240,    364,  81679,  14325],\n",
            "        [  2622,   2395,    412, 141068,    765,   1294,   1838,     11],\n",
            "        [  8773,    316,    413,  17055,   1165,   2009,    734,  25614],\n",
            "        [    40,   3042,    734,  78368,  25367,     11,    538,  35115]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  5030,    382,   1775,  16240,    364,  81679,  14325,   3042],\n",
            "        [  2395,    412, 141068,    765,   1294,   1838,     11,    483],\n",
            "        [   316,    413,  17055,   1165,   2009,    734,  25614,  83062],\n",
            "        [  3042,    734,  78368,  25367,     11,    538,  35115,  48103]])\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "when input is [976] the target: 5030\n",
            "when input is [976, 5030] the target: 382\n",
            "when input is [976, 5030, 382] the target: 1775\n",
            "when input is [976, 5030, 382, 1775] the target: 16240\n",
            "when input is [976, 5030, 382, 1775, 16240] the target: 364\n",
            "when input is [976, 5030, 382, 1775, 16240, 364] the target: 81679\n",
            "when input is [976, 5030, 382, 1775, 16240, 364, 81679] the target: 14325\n",
            "when input is [976, 5030, 382, 1775, 16240, 364, 81679, 14325] the target: 3042\n",
            "when input is [2622] the target: 2395\n",
            "when input is [2622, 2395] the target: 412\n",
            "when input is [2622, 2395, 412] the target: 141068\n",
            "when input is [2622, 2395, 412, 141068] the target: 765\n",
            "when input is [2622, 2395, 412, 141068, 765] the target: 1294\n",
            "when input is [2622, 2395, 412, 141068, 765, 1294] the target: 1838\n",
            "when input is [2622, 2395, 412, 141068, 765, 1294, 1838] the target: 11\n",
            "when input is [2622, 2395, 412, 141068, 765, 1294, 1838, 11] the target: 483\n",
            "when input is [8773] the target: 316\n",
            "when input is [8773, 316] the target: 413\n",
            "when input is [8773, 316, 413] the target: 17055\n",
            "when input is [8773, 316, 413, 17055] the target: 1165\n",
            "when input is [8773, 316, 413, 17055, 1165] the target: 2009\n",
            "when input is [8773, 316, 413, 17055, 1165, 2009] the target: 734\n",
            "when input is [8773, 316, 413, 17055, 1165, 2009, 734] the target: 25614\n",
            "when input is [8773, 316, 413, 17055, 1165, 2009, 734, 25614] the target: 83062\n",
            "when input is [40] the target: 3042\n",
            "when input is [40, 3042] the target: 734\n",
            "when input is [40, 3042, 734] the target: 78368\n",
            "when input is [40, 3042, 734, 78368] the target: 25367\n",
            "when input is [40, 3042, 734, 78368, 25367] the target: 11\n",
            "when input is [40, 3042, 734, 78368, 25367, 11] the target: 538\n",
            "when input is [40, 3042, 734, 78368, 25367, 11, 538] the target: 35115\n",
            "when input is [40, 3042, 734, 78368, 25367, 11, 538, 35115] the target: 48103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqXHC7CWNanO",
        "outputId": "e334c719-f6e8-4949-e849-d3e7a149d4a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   976,   5030,    382,   1775,  16240,    364,  81679,  14325],\n",
            "        [  2622,   2395,    412, 141068,    765,   1294,   1838,     11],\n",
            "        [  8773,    316,    413,  17055,   1165,   2009,    734,  25614],\n",
            "        [    40,   3042,    734,  78368,  25367,     11,    538,  35115]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is an Embedding?**\n",
        "*In the context of machine learning, an embedding is a mapping from discrete objects (like words, users, or items) to vectors of real numbers. The idea is to transform high-dimensional, sparse data (like one-hot encodings for words) into lower-dimensional, dense representations. These dense vectors capture semantic relationships, meaning that words with similar meanings will have similar embedding vectors in the vector space.*\n",
        "\n",
        "Why use nn.Embedding? **bold text**\n",
        "Dimensionality Reduction: One-hot encoding for a large vocabulary results in very high-dimensional and sparse vectors. For example, if you have 10,000 unique words, each word would be represented by a 10,000-dimensional vector with a single 1 and 9,999 0s. Embeddings reduce this to a much smaller, dense vector (e.g., 128 or 300 dimensions).\n",
        "Capturing Semantic Meaning: Unlike one-hot encoding, where every word is equidistant from every other word, embeddings learn relationships. For example, the embedding for \"king\" might be very close to \"queen\" in the vector space, and the vector difference king - man + woman might be close to queen *italicized text*. *italicized text*"
      ],
      "metadata": {
        "id": "lWy6cZwnRWqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(len(char))\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(enc.decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "0mthcWE-Op_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dad37c"
      },
      "source": [
        "# Task\n",
        "Define essential training hyperparameters, instantiate an AdamW optimizer for the model, and create a function to estimate loss on both training and validation sets. Then, implement a comprehensive training loop to update model weights and periodically evaluate loss. After training, generate a sample text sequence using the `BiagramLanguageModel`'s `generate` method. Finally, review the training process and generated text to assess model performance and provide insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9e18e25"
      },
      "source": [
        "## Define Hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Set up essential training hyperparameters such as learning rate, number of training iterations, and an evaluation interval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3855b504"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining essential training hyperparameters. I will set the learning rate, maximum iterations, evaluation interval, and evaluation iterations in a code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fd34839",
        "outputId": "06b76976-c132-4415-b5de-76f28a1d6890"
      },
      "source": [
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "lr = 1e-3\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "\n",
        "print(f\"Learning Rate: {lr}\")\n",
        "print(f\"Maximum Iterations: {max_iters}\")\n",
        "print(f\"Evaluation Interval: {eval_interval}\")\n",
        "print(f\"Evaluation Iterations: {eval_iters}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.001\n",
            "Maximum Iterations: 3000\n",
            "Evaluation Interval: 300\n",
            "Evaluation Iterations: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1456c505"
      },
      "source": [
        "## Instantiate Optimizer\n",
        "\n",
        "### Subtask:\n",
        "Instantiate an `AdamW` optimizer for the `BiagramLanguageModel` using the defined learning rate.\n",
        "\n",
        "#### Instructions\n",
        "1. Initialize the `AdamW` optimizer, passing the model's parameters and the previously defined learning rate `lr`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ef70281"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step defined the hyperparameters. Now, as per the current subtask, I need to instantiate an AdamW optimizer for the model using the defined learning rate. This requires a code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3fa400f"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `NameError: name 'torch' is not defined` indicates that the `torch` library was not imported in the current execution environment. I will add `import torch` at the beginning of the code block to ensure `torch` is available when instantiating the optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "afcc55b3",
        "outputId": "cd46b41d-674b-4c0c-80f7-aa6c9eb09a3f"
      },
      "source": [
        "import torch\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)\n",
        "print(f\"Optimizer instantiated with learning rate: {lr}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3477500316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimizer instantiated with learning rate: {lr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89288a9e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "\n",
        "# Re-define enc and m's dependencies as they were not executed in prior cells\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "# Re-define BiagramLanguageModel class as it was not executed\n",
        "class BiagramLanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def  forward(self,idx,targets=None):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "    return logits,loss\n",
        "\n",
        "# Instantiate the model with the correct tiktoken vocabulary size\n",
        "m = BiagramLanguageModel(enc.n_vocab)\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)\n",
        "print(f\"Optimizer instantiated with learning rate: {lr}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}